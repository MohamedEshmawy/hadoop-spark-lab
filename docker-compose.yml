version: '3.8'

# Hadoop/Spark Teaching Lab - Mini Cluster
# Simulates a realistic distributed environment on a single PC

services:
  # ============== HDFS Services ==============
  namenode:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"   # HDFS Web UI
      - "9000:9000"   # HDFS RPC
    volumes:
      - ./namenode_data:/hadoop/dfs/name
      - ./data:/data
      - ./exercises:/exercises
    environment:
      - CLUSTER_NAME=teaching-lab
      - HADOOP_NODE_TYPE=namenode
    env_file:
      - ./docker/hadoop/hadoop.env
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 10s
      timeout: 5s
      retries: 30

  datanode1:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: datanode1
    hostname: datanode1
    ports:
      - "9864:9864"   # DataNode Web UI
    volumes:
      - ./datanode1_data:/hadoop/dfs/data
    environment:
      - HADOOP_NODE_TYPE=datanode
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  datanode2:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: datanode2
    hostname: datanode2
    ports:
      - "9865:9864"
    volumes:
      - ./datanode2_data:/hadoop/dfs/data
    environment:
      - HADOOP_NODE_TYPE=datanode
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  datanode3:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: datanode3
    hostname: datanode3
    ports:
      - "9866:9864"
    volumes:
      - ./datanode3_data:/hadoop/dfs/data
    environment:
      - HADOOP_NODE_TYPE=datanode
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== YARN Services ==============
  resourcemanager:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"   # ResourceManager Web UI
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
    environment:
      - HADOOP_NODE_TYPE=resourcemanager
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088"]
      interval: 30s
      timeout: 10s
      retries: 5

  nodemanager1:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: nodemanager1
    hostname: nodemanager1
    ports:
      - "8042:8042"   # NodeManager Web UI
    environment:
      - HADOOP_NODE_TYPE=nodemanager
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network

  nodemanager2:
    image: hadoop-spark-lab/hadoop:latest
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: nodemanager2
    hostname: nodemanager2
    ports:
      - "8043:8042"
    environment:
      - HADOOP_NODE_TYPE=nodemanager
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== Spark & History Server ==============
  spark-history:
    image: hadoop-spark-lab/spark:latest
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: spark-history
    hostname: spark-history
    ports:
      - "18080:18080"  # Spark History Server UI
    environment:
      - SPARK_MODE=history
    env_file:
      - ./docker/spark/spark.env
    volumes:
      - spark_logs:/spark-logs
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== Jupyter Lab with PySpark ==============
  jupyter:
    image: hadoop-spark-lab/jupyter:latest
    build:
      context: ./docker/jupyter
      dockerfile: Dockerfile
    container_name: jupyter
    hostname: jupyter
    ports:
      - "8888:8888"   # Jupyter Lab UI
      - "4040:4040"   # Spark Application UI (driver)
      - "4041:4041"   # Additional Spark UI
    environment:
      - JUPYTER_TOKEN=hadooplab
      - PYSPARK_PYTHON=/opt/conda/bin/python3
      - PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python3
      - "PYSPARK_SUBMIT_ARGS=--master yarn --deploy-mode client --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/opt/conda/bin/python3 --conf spark.executorEnv.PYSPARK_PYTHON=/opt/conda/bin/python3 pyspark-shell"
    env_file:
      - ./docker/spark/spark.env
    volumes:
      - ./notebooks:/home/jovyan/notebooks
      - ./data:/home/jovyan/data
      - ./exercises:/home/jovyan/exercises
      - spark_logs:/spark-logs
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== PostgreSQL (for Hive) ==============
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    hostname: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init-databases.sql:/docker-entrypoint-initdb.d/init-databases.sql
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============== Hive Services ==============
  hive-metastore:
    image: hadoop-spark-lab/hive:latest
    build:
      context: ./docker/hive
      dockerfile: Dockerfile
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - "9083:9083"    # Thrift Metastore
    environment:
      - HIVE_MODE=metastore
    volumes:
      - ./data:/data
    depends_on:
      namenode:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9083"]
      interval: 30s
      timeout: 10s
      retries: 5

  hiveserver2:
    image: hadoop-spark-lab/hive:latest
    build:
      context: ./docker/hive
      dockerfile: Dockerfile
    container_name: hiveserver2
    hostname: hiveserver2
    ports:
      - "10000:10000"  # JDBC/ODBC
      - "10002:10002"  # HiveServer2 Web UI
    environment:
      - HIVE_MODE=hiveserver2
    volumes:
      - ./data:/data
      - ./exercises:/exercises
    depends_on:
      hive-metastore:
        condition: service_healthy
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "10000"]
      interval: 30s
      timeout: 10s
      retries: 10

  # ============== PostgreSQL Web UI ==============
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    hostname: pgadmin
    ports:
      - "5050:80"    # pgAdmin Web UI
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@hadooplab.com
      - PGADMIN_DEFAULT_PASSWORD=admin
      - PGADMIN_CONFIG_SERVER_MODE=False
      - PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION=False
      - PGADMIN_CONFIG_COOKIE_DEFAULT_SECURE=False
      - PGADMIN_CONFIG_COOKIE_DEFAULT_SAMESITE='Lax'
      - PGADMIN_CONFIG_PROXY_X_FOR_COUNT=1
      - PGADMIN_CONFIG_PROXY_X_PROTO_COUNT=1
      - PGADMIN_CONFIG_PROXY_X_HOST_COUNT=1
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./docker/postgres/servers.json:/var/lib/pgadmin/storage/pgadmin_server_group/servers.json
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - hadoop-network

networks:
  hadoop-network:
    name: hadoopnet
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  spark_logs:
  postgres_data:
  pgadmin_data:







