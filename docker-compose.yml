version: '3.8'

# Hadoop/Spark Teaching Lab - Mini Cluster
# Simulates a realistic distributed environment on a single PC

services:
  # ============== HDFS Services ==============
  namenode:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"   # HDFS Web UI
      - "9000:9000"   # HDFS RPC
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./data:/data
      - ./exercises:/exercises
    environment:
      - CLUSTER_NAME=teaching-lab
      - HADOOP_NODE_TYPE=namenode
    env_file:
      - ./docker/hadoop/hadoop.env
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5

  datanode1:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: datanode1
    hostname: datanode1
    ports:
      - "9864:9864"   # DataNode Web UI
    volumes:
      - datanode1_data:/hadoop/dfs/data
    environment:
      - HADOOP_NODE_TYPE=datanode
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  datanode2:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: datanode2
    hostname: datanode2
    ports:
      - "9865:9864"
    volumes:
      - datanode2_data:/hadoop/dfs/data
    environment:
      - HADOOP_NODE_TYPE=datanode
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  datanode3:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: datanode3
    hostname: datanode3
    ports:
      - "9866:9864"
    volumes:
      - datanode3_data:/hadoop/dfs/data
    environment:
      - HADOOP_NODE_TYPE=datanode
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== YARN Services ==============
  resourcemanager:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"   # ResourceManager Web UI
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
    environment:
      - HADOOP_NODE_TYPE=resourcemanager
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088"]
      interval: 30s
      timeout: 10s
      retries: 5

  nodemanager1:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: nodemanager1
    hostname: nodemanager1
    ports:
      - "8042:8042"   # NodeManager Web UI
    environment:
      - HADOOP_NODE_TYPE=nodemanager
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network

  nodemanager2:
    build:
      context: ./docker/hadoop
      dockerfile: Dockerfile
    container_name: nodemanager2
    hostname: nodemanager2
    ports:
      - "8043:8042"
    environment:
      - HADOOP_NODE_TYPE=nodemanager
    env_file:
      - ./docker/hadoop/hadoop.env
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== Spark & History Server ==============
  spark-history:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: spark-history
    hostname: spark-history
    ports:
      - "18080:18080"  # Spark History Server UI
    environment:
      - SPARK_MODE=history
    env_file:
      - ./docker/spark/spark.env
    volumes:
      - spark_logs:/spark-logs
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  # ============== Spark Client (for spark-submit) ==============
  spark-client:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: spark-client
    hostname: spark-client
    environment:
      - SPARK_MODE=client
    env_file:
      - ./docker/spark/spark.env
    volumes:
      - ./data:/data
      - ./exercises:/exercises
      - spark_logs:/spark-logs
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network
    stdin_open: true
    tty: true

  # ============== Jupyter Lab with PySpark ==============
  jupyter:
    build:
      context: ./docker/jupyter
      dockerfile: Dockerfile
    container_name: jupyter
    hostname: jupyter
    ports:
      - "8888:8888"   # Jupyter Lab UI
      - "4040:4040"   # Spark Application UI (driver)
      - "4041:4041"   # Additional Spark UI
    environment:
      - JUPYTER_TOKEN=hadooplab
    env_file:
      - ./docker/spark/spark.env
    volumes:
      - ./notebooks:/home/jovyan/notebooks
      - ./data:/home/jovyan/data
      - ./exercises:/home/jovyan/exercises
      - spark_logs:/spark-logs
    depends_on:
      resourcemanager:
        condition: service_healthy
    networks:
      - hadoop-network

networks:
  hadoop-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  datanode3_data:
  spark_logs:

