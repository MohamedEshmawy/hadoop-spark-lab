{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Break/Fix Challenge üîß\n",
    "\n",
    "## Learning Objectives\n",
    "- Diagnose cluster failures using monitoring tools\n",
    "- Understand HDFS fault tolerance in action\n",
    "- Observe Spark resilience when executors fail\n",
    "- Practice real-world troubleshooting skills\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Note\n",
    "This exercise involves intentionally breaking components. Make sure you understand how to restart services before proceeding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: DataNode Failure\n",
    "\n",
    "### Scenario\n",
    "A DataNode in your cluster has failed. Your task is to:\n",
    "1. Observe the failure in the HDFS UI\n",
    "2. Verify data is still accessible\n",
    "3. Watch the replication recovery process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check current cluster status\n",
    "!hdfs dfsadmin -report | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Record the current state\n",
    "Open http://localhost:9870 and note:\n",
    "- How many live DataNodes?\n",
    "- What's the block replication status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check block locations for our test file\n",
    "!hdfs fsck /user/student/data/transactions.csv -files -blocks -locations 2>/dev/null | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Simulate DataNode Failure\n",
    "Run this in a terminal (not in this notebook):\n",
    "```bash\n",
    "docker stop datanode1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 30 seconds, then check status again\n",
    "import time\n",
    "print(\"Waiting for NameNode to detect failure...\")\n",
    "time.sleep(30)\n",
    "!hdfs dfsadmin -report | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checkpoint Question 5a\n",
    "Look at the HDFS UI (http://localhost:9870):\n",
    "- How many DataNodes are now shown as live?\n",
    "- Are there any under-replicated blocks?\n",
    "- Can you still read the transactions file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to read the file - does it work?\n",
    "!hdfs dfs -head /user/student/data/transactions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "- Live DataNodes: \n",
    "- Under-replicated blocks: \n",
    "- File readable: Yes/No\n",
    "- Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Recovery\n",
    "```bash\n",
    "docker start datanode1\n",
    "```\n",
    "Wait 1 minute and observe the HDFS UI - watch replicas recover!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify recovery\n",
    "print(\"Waiting for DataNode to rejoin...\")\n",
    "time.sleep(60)\n",
    "!hdfs dfsadmin -report | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge 2: Spark Job Resilience\n",
    "\n",
    "### Scenario\n",
    "An executor fails during a long-running Spark job. Observe how Spark handles the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Resilience Test\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.task.maxFailures\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"App ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "df.cache()\n",
    "df.count()  # Materialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Start a Long-Running Job\n",
    "Run the next cell, then quickly kill a NodeManager:\n",
    "```bash\n",
    "docker stop nodemanager1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-running aggregation (run this, then kill nodemanager1)\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "result = df.repartition(8) \\\n",
    "    .groupBy(\"store_region\", \"payment_method\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        sum(\"total_amount\").alias(\"total\"),\n",
    "        avg(\"quantity\").alias(\"avg_qty\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total\"))\n",
    "\n",
    "result.show(20)\n",
    "print(f\"Completed in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checkpoint Question 5b\n",
    "Look at the Spark UI (http://localhost:4040 or YARN UI):\n",
    "- Did the job complete successfully?\n",
    "- Were any tasks retried?\n",
    "- How did Spark recover from the lost executor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Recovery\n",
    "```bash\n",
    "docker start nodemanager1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Questions\n",
    "\n",
    "1. Why did HDFS continue to work with one DataNode down?\n",
    "2. What would happen if replication factor was 1 and that DataNode failed?\n",
    "3. How does Spark's RDD lineage help with fault tolerance?\n",
    "4. What's the difference between executor failure and driver failure?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
