{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Spark and Hive Integration\n",
    "\n",
    "## The Power of Unified Metadata\n",
    "\n",
    "One of Spark's most powerful features is its ability to read and write tables from the **Hive Metastore**. This means:\n",
    "\n",
    "- Tables created in Hive can be queried by Spark\n",
    "- Tables created by Spark are visible to Hive\n",
    "- Other tools (Presto, Trino, Flink) can also access the same tables\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│                  HIVE METASTORE                         │\n",
    "│              (Single Source of Truth)                   │\n",
    "└────────────────────────┬─────────────────────────────────┘\n",
    "                         │\n",
    "         ┌───────────────┼───────────────┐\n",
    "         │               │               │\n",
    "         ▼               ▼               ▼\n",
    "    ┌─────────┐    ┌─────────┐    ┌─────────┐\n",
    "    │  Spark  │    │  Hive   │    │ Presto  │\n",
    "    │ (Fast!) │    │  (SQL)  │    │ (OLAP)  │\n",
    "    └─────────┘    └─────────┘    └─────────┘\n",
    "         │               │               │\n",
    "         └───────────────┼───────────────┘\n",
    "                         ▼\n",
    "                    ┌─────────┐\n",
    "                    │  HDFS   │\n",
    "                    │  Data   │\n",
    "                    └─────────┘\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "- Connect Spark to the Hive Metastore\n",
    "- Read tables created by Hive using Spark\n",
    "- Create tables from Spark that are visible in Hive\n",
    "- Use Spark's DataFrame API with Hive tables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Connecting Spark to Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark connected to Hive Metastore!\n",
      "Application ID: application_1768170757723_0001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Key configuration: enableHiveSupport() and metastore URI\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-Hive Integration Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark connected to Hive Metastore!\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Available Databases ===\n",
      "+---------------+\n",
      "|namespace      |\n",
      "+---------------+\n",
      "|default        |\n",
      "|sales_analytics|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify connection by listing databases\n",
    "print(\"=== Available Databases ===\")\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Accessing Hive Tables from Spark\n",
    "\n",
    "Tables in Hive are seamlessly accessible via Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_hive_demo\")\n",
    "spark.sql(\"USE spark_hive_demo\")\n",
    "\n",
    "# Create a sample table (this will be visible in Hive!)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS employees (\n",
    "        emp_id INT,\n",
    "        name STRING,\n",
    "        department STRING,\n",
    "        salary DECIMAL(10,2),\n",
    "        hire_date DATE\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table 'employees' created in Hive Metastore!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data using Spark DataFrame API\n",
    "from datetime import date\n",
    "\n",
    "employee_data = [\n",
    "    (1, \"Alice Johnson\", \"Engineering\", 85000.00, date(2020, 3, 15)),\n",
    "    (2, \"Bob Smith\", \"Marketing\", 72000.00, date(2019, 7, 1)),\n",
    "    (3, \"Carol Williams\", \"Engineering\", 92000.00, date(2018, 11, 20)),\n",
    "    (4, \"David Brown\", \"Sales\", 68000.00, date(2021, 1, 10)),\n",
    "    (5, \"Eva Martinez\", \"Engineering\", 95000.00, date(2017, 5, 5)),\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame(\n",
    "    employee_data,\n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "# Write to Hive table\n",
    "emp_df.write.mode(\"overwrite\").saveAsTable(\"employees\")\n",
    "print(\"Data inserted into employees table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table - works the same in Spark AND Hive!\n",
    "print(\"=== Employees Table (via Spark SQL) ===\")\n",
    "spark.sql(\"SELECT * FROM employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using DataFrame API with Hive Tables\n",
    "\n",
    "The real power comes from combining Spark's DataFrame API with Hive tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Hive table as DataFrame\n",
    "employees_df = spark.table(\"employees\")\n",
    "\n",
    "# Use DataFrame operations\n",
    "from pyspark.sql.functions import avg, count, col, max as spark_max\n",
    "\n",
    "# Department statistics\n",
    "dept_stats = employees_df \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"employee_count\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\"),\n",
    "        spark_max(\"salary\").alias(\"max_salary\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "print(\"=== Department Statistics ===\")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated results as a new Hive table\n",
    "dept_stats.write.mode(\"overwrite\").saveAsTable(\"department_summary\")\n",
    "\n",
    "print(\"Aggregated data saved as new Hive table!\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating Partitioned Tables with Spark\n",
    "\n",
    "Spark can create and write to partitioned Hive tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales data with dates\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "sales_data = [\n",
    "    (\"S001\", \"Widget A\", 100, 29.99, datetime(2024, 1, 15)),\n",
    "    (\"S002\", \"Widget B\", 50, 49.99, datetime(2024, 1, 20)),\n",
    "    (\"S003\", \"Widget A\", 75, 29.99, datetime(2024, 2, 5)),\n",
    "    (\"S004\", \"Widget C\", 200, 19.99, datetime(2024, 2, 10)),\n",
    "    (\"S005\", \"Widget B\", 30, 49.99, datetime(2024, 3, 1)),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"sale_id\", \"product\", \"quantity\", \"unit_price\", \"sale_date\"]\n",
    ")\n",
    "\n",
    "# Add partition columns\n",
    "sales_df = sales_df \\\n",
    "    .withColumn(\"sale_year\", year(\"sale_date\")) \\\n",
    "    .withColumn(\"sale_month\", month(\"sale_date\"))\n",
    "\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write as partitioned Hive table\n",
    "sales_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"sale_year\", \"sale_month\") \\\n",
    "    .saveAsTable(\"sales_partitioned\")\n",
    "\n",
    "print(\"Partitioned table created!\")\n",
    "spark.sql(\"SHOW PARTITIONS sales_partitioned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with partition pruning\n",
    "print(\"=== January 2024 Sales ===\")\n",
    "jan_sales = spark.sql(\"\"\"\n",
    "    SELECT sale_id, product, quantity, unit_price, sale_date\n",
    "    FROM sales_partitioned\n",
    "    WHERE sale_year = 2024 AND sale_month = 1\n",
    "\"\"\")\n",
    "jan_sales.show()\n",
    "\n",
    "# Verify partition pruning in query plan\n",
    "print(\"\\n=== Query Plan (Notice PartitionFilters) ===\")\n",
    "jan_sales.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Verifying Cross-Tool Access\n",
    "\n",
    "Tables created by Spark are visible to Hive and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table metadata - same query works in HiveServer2\n",
    "print(\"=== Table Details (visible in Hive too!) ===\")\n",
    "spark.sql(\"DESCRIBE FORMATTED employees\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run the same query in Hive CLI:\n",
    "# beeline -u \"jdbc:hive2://hiveserver2:10000/spark_hive_demo\"\n",
    "# > SELECT * FROM employees;\n",
    "\n",
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════╗\n",
    "║  To verify in Hive, run in terminal:                          ║\n",
    "║                                                                ║\n",
    "║  docker exec -it hiveserver2 beeline -u \\\\                     ║\n",
    "║    \"jdbc:hive2://localhost:10000/spark_hive_demo\"              ║\n",
    "║                                                                ║\n",
    "║  Then: SELECT * FROM employees;                                ║\n",
    "╚════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Integration Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Join Hive tables in Spark\n",
    "print(\"=== Joining Hive Tables in Spark ===\")\n",
    "\n",
    "# Create a departments reference table\n",
    "dept_data = [\n",
    "    (\"Engineering\", \"Building A\", 50),\n",
    "    (\"Marketing\", \"Building B\", 25),\n",
    "    (\"Sales\", \"Building C\", 40),\n",
    "]\n",
    "\n",
    "dept_df = spark.createDataFrame(\n",
    "    dept_data,\n",
    "    [\"dept_name\", \"location\", \"budget_millions\"]\n",
    ")\n",
    "dept_df.write.mode(\"overwrite\").saveAsTable(\"departments\")\n",
    "\n",
    "# Join employees with departments\n",
    "joined = spark.sql(\"\"\"\n",
    "    SELECT e.name, e.department, e.salary, d.location\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.dept_name\n",
    "\"\"\")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Insert data into existing Hive table\n",
    "new_employee = [(6, \"Frank Lee\", \"Marketing\", 75000.00, date(2024, 1, 15))]\n",
    "new_emp_df = spark.createDataFrame(\n",
    "    new_employee,\n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "# Append to existing table\n",
    "new_emp_df.write.mode(\"append\").saveAsTable(\"employees\")\n",
    "\n",
    "print(\"=== Updated Employees ===\")\n",
    "spark.sql(\"SELECT * FROM employees ORDER BY emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: CTAS (Create Table As Select)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS high_earners AS\n",
    "    SELECT emp_id, name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 80000\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== High Earners Table ===\")\n",
    "spark.sql(\"SELECT * FROM high_earners\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Spark + Hive Integration Benefits\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| Unified Catalog | Single source of truth for all tools |\n",
    "| Schema on Read | Tables are always consistent |\n",
    "| Partition Pruning | Efficient queries on large datasets |\n",
    "| Parallel Processing | Spark's fast execution engine |\n",
    "| SQL Compatibility | Same queries work in Spark and Hive |\n",
    "| DataFrame API | Python/Scala programmatic access |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS employees\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS departments\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS department_summary\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS sales_partitioned\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS high_earners\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS spark_hive_demo CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
