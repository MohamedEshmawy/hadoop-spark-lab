{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Spark Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the difference between RDDs and DataFrames\n",
    "- Master transformations (lazy) vs actions (eager)\n",
    "- Create and manipulate distributed datasets\n",
    "- View job execution in the Spark UI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession connected to YARN\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Fundamentals Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”— **Open Spark UI**: http://localhost:4040 to monitor your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Working with RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "RDDs are the foundational data structure in Spark - immutable, distributed collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a Python list\n",
    "numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], numSlices=4)\n",
    "print(f\"Number of partitions: {numbers.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations are LAZY - they don't execute immediately\n",
    "squared = numbers.map(lambda x: x ** 2)\n",
    "filtered = squared.filter(lambda x: x > 20)\n",
    "\n",
    "print(\"Transformations defined, but nothing executed yet!\")\n",
    "print(f\"Type: {type(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions TRIGGER execution\n",
    "result = filtered.collect()\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Checkpoint Question 2\n",
    "Check the Spark UI (Jobs tab). How many jobs were triggered by the code above?\n",
    "Which operation triggered the job - `map()`, `filter()`, or `collect()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Common RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file as RDD\n",
    "lines_rdd = spark.sparkContext.textFile(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "# Skip header and parse\n",
    "header = lines_rdd.first()\n",
    "data_rdd = lines_rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "def parse_transaction(line):\n",
    "    fields = line.split(',')\n",
    "    return {\n",
    "        'transaction_id': fields[0],\n",
    "        'amount': float(fields[7]) if len(fields) > 7 else 0.0\n",
    "    }\n",
    "\n",
    "parsed = data_rdd.map(parse_transaction)\n",
    "amounts = parsed.map(lambda x: x['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "print(f\"Count: {amounts.count()}\")\n",
    "print(f\"Sum: ${amounts.sum():,.2f}\")\n",
    "print(f\"Average: ${amounts.mean():,.2f}\")\n",
    "print(f\"Max: ${amounts.max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: DataFrames - The Modern API\n",
    "\n",
    "DataFrames are structured, schema-aware, and optimized with Catalyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV with schema inference\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame transformations\n",
    "from pyspark.sql.functions import col, sum, avg, count\n",
    "\n",
    "# Group by region and calculate totals\n",
    "region_summary = df.groupBy(\"store_region\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_sales\", ascending=False)\n",
    "\n",
    "region_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding Lazy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all transformations - nothing executes yet!\n",
    "step1 = df.filter(col(\"is_online\") == True)\n",
    "step2 = step1.select(\"transaction_id\", \"total_amount\", \"payment_method\")\n",
    "step3 = step2.filter(col(\"total_amount\") > 100)\n",
    "\n",
    "print(\"Built the execution plan. Check Spark UI - no new jobs yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the execution plan\n",
    "step3.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger execution with an action\n",
    "result_count = step3.count()\n",
    "print(f\"High-value online transactions: {result_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession when done\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

