{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Spark Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the difference between RDDs and DataFrames\n",
    "- Master transformations (lazy) vs actions (eager)\n",
    "- Create and manipulate distributed datasets\n",
    "- View job execution in the Spark UI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Application ID: application_1768525703064_0001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession connected to YARN\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Fundamentals Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”— **Open Spark UI**: http://localhost:4040 to monitor your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Working with RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "RDDs are the foundational data structure in Spark - immutable, distributed collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a Python list\n",
    "numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], numSlices=2)\n",
    "print(f\"Number of partitions: {numbers.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations defined, but nothing executed yet!\n",
      "Type: <class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# Transformations are LAZY - they don't execute immediately\n",
    "squared = numbers.map(lambda x: x ** 2)\n",
    "filtered = squared.filter(lambda x: x > 20)\n",
    "\n",
    "print(\"Transformations defined, but nothing executed yet!\")\n",
    "print(f\"Type: {type(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "# Actions TRIGGER execution\n",
    "result = filtered.collect()\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Checkpoint Question 2\n",
    "Check the Spark UI (Jobs tab). How many jobs were triggered by the code above?\n",
    "Which operation triggered the job - `map()`, `filter()`, or `collect()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Common RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file as RDD\n",
    "lines_rdd = spark.sparkContext.textFile(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "# Skip header and parse\n",
    "data_rdd = lines_rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "def parse_transaction(line):\n",
    "    fields = line.split(',')\n",
    "    return {\n",
    "        'transaction_id': fields[0],\n",
    "        'amount': float(fields[7]) if len(fields) > 7 else 0.0\n",
    "    }\n",
    "\n",
    "parsed = data_rdd.map(parse_transaction)\n",
    "amounts = parsed.map(lambda x: x['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 100000\n",
      "Sum: $130,918,536.52\n",
      "Average: $1,309.19\n",
      "Max: $4,990.80\n"
     ]
    }
   ],
   "source": [
    "# Actions\n",
    "print(f\"Count: {amounts.count()}\")\n",
    "print(f\"Sum: ${amounts.sum():,.2f}\")\n",
    "print(f\"Average: ${amounts.mean():,.2f}\")\n",
    "print(f\"Max: ${amounts.max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: DataFrames - The Modern API\n",
    "\n",
    "DataFrames are structured, schema-aware, and optimized with Catalyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_time: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- store_region: string (nullable = true)\n",
      " |-- is_online: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV with schema inference\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+-----------+----------+--------+----------+------------+--------------+------------+---------+\n",
      "|transaction_id|transaction_date|   transaction_time|customer_id|product_id|quantity|unit_price|total_amount|payment_method|store_region|is_online|\n",
      "+--------------+----------------+-------------------+-----------+----------+--------+----------+------------+--------------+------------+---------+\n",
      "|   TXN00000001|      2024-06-03|2026-01-11 11:56:54| CUST000481| PROD00352|       1|    259.06|      259.06|        PayPal|       North|     true|\n",
      "|   TXN00000002|      2023-11-22|2026-01-11 14:25:51| CUST001581| PROD00288|       6|     442.7|      2656.2|        PayPal|     Central|    false|\n",
      "|   TXN00000003|      2023-08-13|2026-01-11 13:40:52| CUST000534| PROD00205|       1|     116.0|       116.0|   Credit Card|     Central|    false|\n",
      "|   TXN00000004|      2023-06-20|2026-01-11 16:13:49| CUST000840| PROD00230|       4|    187.53|      750.12|   Credit Card|       North|    false|\n",
      "|   TXN00000005|      2024-01-24|2026-01-11 09:49:45| CUST001239| PROD00067|       9|      5.14|       46.28|          Cash|       South|    false|\n",
      "+--------------+----------------+-------------------+-----------+----------+--------+----------+------------+--------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------------------+------------------+\n",
      "|store_region|transaction_count|         total_sales|   avg_transaction|\n",
      "+------------+-----------------+--------------------+------------------+\n",
      "|        West|            21575|2.8312219759999976E7| 1312.269745538817|\n",
      "|       North|            20048| 2.652123458999993E7|1322.8868011771713|\n",
      "|       South|            20191| 2.649756461000003E7|1312.3453325739206|\n",
      "|     Central|            19364|2.5204887519999914E7|1301.6364139640525|\n",
      "|        East|            18822|2.4382630039999966E7|1295.4324747635726|\n",
      "+------------+-----------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame transformations\n",
    "from pyspark.sql.functions import col, sum, avg, count\n",
    "\n",
    "# Group by region and calculate totals\n",
    "region_summary = df.groupBy(\"store_region\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_sales\", ascending=False)\n",
    "\n",
    "region_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding Lazy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built the execution plan. Check Spark UI - no new jobs yet!\n"
     ]
    }
   ],
   "source": [
    "# These are all transformations - nothing executes yet!\n",
    "step1 = df.filter(col(\"is_online\") == True)\n",
    "step2 = step1.select(\"transaction_id\", \"total_amount\", \"payment_method\")\n",
    "step3 = step2.filter(col(\"total_amount\") > 100)\n",
    "\n",
    "print(\"Built the execution plan. Check Spark UI - no new jobs yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('total_amount > 100)\n",
      "+- Project [transaction_id#17, total_amount#24, payment_method#25]\n",
      "   +- Filter (is_online#27 = true)\n",
      "      +- Relation [transaction_id#17,transaction_date#18,transaction_time#19,customer_id#20,product_id#21,quantity#22,unit_price#23,total_amount#24,payment_method#25,store_region#26,is_online#27] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_id: string, total_amount: double, payment_method: string\n",
      "Filter (total_amount#24 > cast(100 as double))\n",
      "+- Project [transaction_id#17, total_amount#24, payment_method#25]\n",
      "   +- Filter (is_online#27 = true)\n",
      "      +- Relation [transaction_id#17,transaction_date#18,transaction_time#19,customer_id#20,product_id#21,quantity#22,unit_price#23,total_amount#24,payment_method#25,store_region#26,is_online#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [transaction_id#17, total_amount#24, payment_method#25]\n",
      "+- Filter ((isnotnull(is_online#27) AND isnotnull(total_amount#24)) AND (is_online#27 AND (total_amount#24 > 100.0)))\n",
      "   +- Relation [transaction_id#17,transaction_date#18,transaction_time#19,customer_id#20,product_id#21,quantity#22,unit_price#23,total_amount#24,payment_method#25,store_region#26,is_online#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [transaction_id#17, total_amount#24, payment_method#25]\n",
      "+- *(1) Filter (((isnotnull(is_online#27) AND isnotnull(total_amount#24)) AND is_online#27) AND (total_amount#24 > 100.0))\n",
      "   +- FileScan csv [transaction_id#17,total_amount#24,payment_method#25,is_online#27] Batched: false, DataFilters: [isnotnull(is_online#27), isnotnull(total_amount#24), is_online#27, (total_amount#24 > 100.0)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/user/student/data/transactions.csv], PartitionFilters: [], PushedFilters: [IsNotNull(is_online), IsNotNull(total_amount), EqualTo(is_online,true), GreaterThan(total_amount..., ReadSchema: struct<transaction_id:string,total_amount:double,payment_method:string,is_online:boolean>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the execution plan\n",
    "step3.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-value online transactions: 47192\n"
     ]
    }
   ],
   "source": [
    "# Trigger execution with an action\n",
    "result_count = step3.count()\n",
    "print(f\"High-value online transactions: {result_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop SparkSession when done\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
