{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Apache Airflow - Workflow Orchestration\n",
    "\n",
    "## Why Airflow?\n",
    "\n",
    "In previous exercises, we ran individual notebooks manually. In production, data pipelines need:\n",
    "\n",
    "- **Scheduling**: Run at specific times (daily, hourly)\n",
    "- **Dependencies**: Job B runs only after Job A completes\n",
    "- **Retries**: Automatic retry on failures\n",
    "- **Monitoring**: Track pipeline health\n",
    "- **Alerting**: Notify on failures\n",
    "\n",
    "**Apache Airflow** solves these challenges!\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    WITHOUT AIRFLOW                             │\n",
    "│                                                                 │\n",
    "│   Cron Job 1    Cron Job 2    Cron Job 3                       │\n",
    "│   (Extract)     (Transform)   (Load)                           │\n",
    "│       │             │            │                             │\n",
    "│       │   Hope they run in order! ❌                           │\n",
    "│       └─────────────┴────────────┘                             │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    WITH AIRFLOW                                │\n",
    "│                                                                 │\n",
    "│   ┌─────────┐    ┌───────────┐    ┌──────────┐                 │\n",
    "│   │ Extract │───▶│ Transform │───▶│   Load   │                 │\n",
    "│   └─────────┘    └───────────┘    └──────────┘                 │\n",
    "│        │               │               │                       │\n",
    "│        ▼               ▼               ▼                       │\n",
    "│   ┌───────────────────────────────────────────┐               │\n",
    "│   │  Airflow Scheduler (Manages Dependencies) │ ✓             │\n",
    "│   └───────────────────────────────────────────┘               │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Airflow concepts (DAGs, Tasks, Operators)\n",
    "- Examine real DAG examples\n",
    "- Trigger and monitor DAG runs\n",
    "- Create pipelines that orchestrate Hive and Spark jobs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Airflow Core Concepts\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|----------|\n",
    "| **DAG** | Directed Acyclic Graph - the pipeline | `daily_sales_etl` |\n",
    "| **Task** | A single unit of work | `extract_sales_data` |\n",
    "| **Operator** | Template for a task type | `BashOperator`, `PythonOperator` |\n",
    "| **Sensor** | Waits for a condition | `FileSensor` (wait for file) |\n",
    "| **XCom** | Share data between tasks | Pass values between operators |\n",
    "| **Connection** | External system credentials | Hive, Spark, S3 connections |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow is running as a service in our cluster\n",
    "# Let's check its status\n",
    "\n",
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════╗\n",
    "║  AIRFLOW WEB UI                                                ║\n",
    "║                                                                ║\n",
    "║  URL: http://localhost:8080                                    ║\n",
    "║  Username: admin                                               ║\n",
    "║  Password: admin                                               ║\n",
    "║                                                                ║\n",
    "║  Open in browser to see DAGs, runs, and logs!                  ║\n",
    "╚════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Anatomy of a DAG\n",
    "\n",
    "Let's examine a real DAG file from our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Hive ETL DAG\n",
    "hive_etl_dag = '''\n",
    "\"\"\"\n",
    "Sample DAG: Hive ETL Pipeline\n",
    "Demonstrates Hive operations from Airflow\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Default arguments applied to all tasks\n",
    "default_args = {\n",
    "    'owner': 'teaching-lab',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=2),\n",
    "}\n",
    "\n",
    "BEELINE_CMD = 'beeline -u \"jdbc:hive2://hiveserver2:10000/default\" --silent=true -e'\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    'hive_etl_example',           # DAG ID\n",
    "    default_args=default_args,\n",
    "    description='Example Hive ETL pipeline',\n",
    "    schedule_interval=None,        # Manual trigger only\n",
    "    catchup=False,\n",
    "    tags=['demo', 'hive', 'etl'],\n",
    ") as dag:\n",
    "    \n",
    "    # Task 1: Create database\n",
    "    create_database = BashOperator(\n",
    "        task_id='create_database',\n",
    "        bash_command=f'{BEELINE_CMD} \"CREATE DATABASE IF NOT EXISTS airflow_demo;\"',\n",
    "    )\n",
    "    \n",
    "    # Task 2: Create table\n",
    "    create_table = BashOperator(\n",
    "        task_id='create_table',\n",
    "        bash_command=f\\'\\'\\'\n",
    "            {BEELINE_CMD} \"\n",
    "                USE airflow_demo;\n",
    "                CREATE TABLE IF NOT EXISTS daily_metrics (\n",
    "                    metric_name STRING,\n",
    "                    metric_value DOUBLE,\n",
    "                    recorded_at TIMESTAMP\n",
    "                )\n",
    "                PARTITIONED BY (date_key STRING)\n",
    "                STORED AS PARQUET;\n",
    "            \"\n",
    "        \\'\\'\\',\n",
    "    )\n",
    "    \n",
    "    # Define task dependencies\n",
    "    create_database >> create_table  # create_table runs AFTER create_database\n",
    "'''\n",
    "\n",
    "print(\"=== Example Hive ETL DAG ===\")\n",
    "print(hive_etl_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Task Dependencies\n",
    "\n",
    "In Airflow, you define execution order using:\n",
    "\n",
    "```python\n",
    "# Method 1: Bit shift operators\n",
    "task_a >> task_b >> task_c  # Linear: A then B then C\n",
    "\n",
    "# Method 2: List syntax\n",
    "[task_a, task_b] >> task_c  # Parallel: A and B then C\n",
    "\n",
    "# Method 3: set_downstream/set_upstream\n",
    "task_a.set_downstream(task_b)\n",
    "task_c.set_upstream(task_b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of DAG dependencies\n",
    "print(\"\"\"\n",
    "HIVE ETL DAG VISUALIZATION\n",
    "\n",
    "  create_database\n",
    "        |\n",
    "        v\n",
    "  create_table\n",
    "        |\n",
    "        v\n",
    "  insert_data\n",
    "        |\n",
    "        v\n",
    "  run_aggregation\n",
    "        |\n",
    "        v\n",
    "  show_partitions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Common Airflow Operators\n",
    "\n",
    "Airflow provides many built-in operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "COMMON AIRFLOW OPERATORS\n",
    "\n",
    "  BashOperator        - Run shell commands\n",
    "  PythonOperator      - Execute Python functions\n",
    "  SparkSubmitOperator - Submit Spark jobs to cluster\n",
    "  HiveOperator        - Execute Hive queries\n",
    "  FileSensor          - Wait for file to appear\n",
    "  EmailOperator       - Send email notifications\n",
    "  BranchOperator      - Conditional execution paths\n",
    "  DummyOperator       - No-op placeholder task\n",
    "  DockerOperator      - Run commands in Docker containers\n",
    "  HTTPSensor          - Wait for HTTP endpoint response\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Interacting with Airflow via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Airflow REST API base URL\n",
    "AIRFLOW_URL = \"http://airflow-webserver:8080/api/v1\"\n",
    "AUTH = (\"admin\", \"admin\")  # Default credentials\n",
    "\n",
    "def list_dags():\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{AIRFLOW_URL}/dags\",\n",
    "            auth=AUTH,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Connection error: {e}\"\n",
    "\n",
    "print(\"=== Available DAGs ===\")\n",
    "dags = list_dags()\n",
    "if isinstance(dags, dict) and 'dags' in dags:\n",
    "    for dag in dags['dags']:\n",
    "        status = \"Active\" if not dag.get('is_paused') else \"Paused\"\n",
    "        print(f\"  {dag['dag_id']}: {status}\")\n",
    "else:\n",
    "    print(f\"Could not connect to Airflow: {dags}\")\n",
    "    print(\"(This is expected if running outside the Docker network)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Pipeline Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "PRODUCTION ETL PIPELINE PATTERN\n",
    "\n",
    "      Daily Schedule Trigger\n",
    "              |\n",
    "              v\n",
    "      Wait for Source File (FileSensor)\n",
    "              |\n",
    "    +---------+---------+\n",
    "    |         |         |\n",
    "    v         v         v\n",
    " Extract   Extract   Extract\n",
    " Orders    Products  Customers\n",
    "    |         |         |\n",
    "    +---------+---------+\n",
    "              |\n",
    "              v\n",
    "      Transform & Join (Spark)\n",
    "              |\n",
    "              v\n",
    "      Load to Hive Table\n",
    "              |\n",
    "    +---------+---------+\n",
    "    |         |         |\n",
    "    v         v         v\n",
    " Run Tests  Notify   Update Stats\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Airflow Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "AIRFLOW BEST PRACTICES\n",
    "\n",
    "1. IDEMPOTENCY\n",
    "   - Tasks should produce same result if run multiple times\n",
    "   - Use OVERWRITE mode, not APPEND\n",
    "   - Use date partitions for incremental loads\n",
    "\n",
    "2. ATOMICITY\n",
    "   - Each task should be a complete unit of work\n",
    "   - Write to temp location, then move atomically\n",
    "\n",
    "3. AVOID STORING DATA IN XCOMS\n",
    "   - XComs are for small metadata only (file paths, counts)\n",
    "   - Store actual data in HDFS/S3/Database\n",
    "\n",
    "4. USE CONNECTIONS & VARIABLES\n",
    "   - Store credentials in Airflow Connections\n",
    "   - Store config in Airflow Variables\n",
    "   - Never hardcode secrets in DAG files\n",
    "\n",
    "5. MEANINGFUL TASK IDS\n",
    "   - extract_sales_data NOT task1\n",
    "   - Makes debugging and monitoring easier\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Troubleshooting Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "COMMON TROUBLESHOOTING COMMANDS\n",
    "\n",
    "# View DAG runs\n",
    "docker exec airflow-webserver airflow dags list-runs -d hive_etl_example\n",
    "\n",
    "# Trigger a DAG manually\n",
    "docker exec airflow-webserver airflow dags trigger hive_etl_example\n",
    "\n",
    "# View task logs\n",
    "docker exec airflow-webserver airflow tasks logs hive_etl_example create_database\n",
    "\n",
    "# Test a single task\n",
    "docker exec airflow-webserver airflow tasks test hive_etl_example create_database 2024-01-01\n",
    "\n",
    "# Clear failed tasks (for retry)\n",
    "docker exec airflow-webserver airflow tasks clear hive_etl_example -t create_table\n",
    "\n",
    "# Pause/unpause a DAG\n",
    "docker exec airflow-webserver airflow dags pause hive_etl_example\n",
    "docker exec airflow-webserver airflow dags unpause hive_etl_example\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Explore the Airflow UI\n",
    "1. Open http://localhost:8080\n",
    "2. Login with admin/admin\n",
    "3. Find the `hive_etl_example` DAG\n",
    "4. Trigger it manually and watch the execution\n",
    "\n",
    "### Exercise 2: Create Your Own DAG\n",
    "Create a DAG that:\n",
    "1. Checks if a Hive table exists\n",
    "2. Runs a Spark aggregation job\n",
    "3. Writes results to a new Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "KEY TAKEAWAYS\n",
    "\n",
    "1. Airflow = Workflow orchestration for data pipelines\n",
    "\n",
    "2. DAGs define:\n",
    "   - WHAT tasks to run\n",
    "   - WHEN to run (schedule)\n",
    "   - In what ORDER (dependencies)\n",
    "\n",
    "3. Operators execute work:\n",
    "   - BashOperator for shell commands\n",
    "   - SparkSubmitOperator for Spark jobs\n",
    "   - HiveOperator for Hive queries\n",
    "\n",
    "4. Integration with Big Data stack:\n",
    "   - Submits jobs to YARN cluster\n",
    "   - Queries Hive tables\n",
    "   - Monitors HDFS for new files\n",
    "\n",
    "5. Production features:\n",
    "   - Retries, alerting, logging\n",
    "   - Backfill historical data\n",
    "   - Parallel task execution\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
