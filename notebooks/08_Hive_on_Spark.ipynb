{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Hive Execution Engines\n",
    "\n",
    "## MapReduce vs Tez vs Spark\n",
    "\n",
    "Hive was originally built to run on **MapReduce**, but it can now use faster execution engines:\n",
    "\n",
    "| Engine | Description | Performance | Use Case |\n",
    "|--------|-------------|-------------|----------|\n",
    "| **MapReduce** | Original Hive engine | Slow (disk I/O heavy) | Legacy, batch |\n",
    "| **Tez** | DAG-based execution | 10x faster | Default on Hortonworks |\n",
    "| **Spark** | In-memory processing | 10-100x faster | Interactive, ML |\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     HIVE QUERY                             │\n",
    "│         SELECT * FROM sales WHERE region = 'US'            │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                         │\n",
    "         ┌───────────────┼───────────────┐\n",
    "         │               │               │\n",
    "         ▼               ▼               ▼\n",
    "    ┌─────────┐    ┌─────────┐    ┌─────────┐\n",
    "    │   MR    │    │   Tez   │    │  Spark  │\n",
    "    │  Slow   │    │  Fast   │    │ Fastest │\n",
    "    │  Disk   │    │   DAG   │    │ Memory  │\n",
    "    └─────────┘    └─────────┘    └─────────┘\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Hive execution engines\n",
    "- Configure Hive to use Spark as execution engine\n",
    "- Compare performance between engines\n",
    "- Know when to use which approach\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Current Hive Configuration\n",
    "\n",
    "Let's first check the current execution engine configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use subprocess to run beeline commands\n",
    "import subprocess\n",
    "\n",
    "def run_hive_query(query, database=\"default\"):\n",
    "    \"\"\"Execute a Hive query using beeline and return the result.\"\"\"\n",
    "    cmd = f'''docker exec hiveserver2 beeline -u \"jdbc:hive2://localhost:10000/{database}\" \\\n",
    "              --silent=true -e \"{query}\"'''\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    return result.stdout\n",
    "\n",
    "print(\"Hive query helper function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we'll primarily demonstrate concepts\n",
    "# and use Spark to show the differences\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive Execution Engines Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created for Hive operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Execution Engines\n",
    "\n",
    "### MapReduce (Default in older Hive)\n",
    "```xml\n",
    "<property>\n",
    "    <name>hive.execution.engine</name>\n",
    "    <value>mr</value>\n",
    "</property>\n",
    "```\n",
    "\n",
    "### Spark (Modern, Fast)\n",
    "```xml\n",
    "<property>\n",
    "    <name>hive.execution.engine</name>\n",
    "    <value>spark</value>\n",
    "</property>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View current hive-site.xml execution engine setting\n",
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════╗\n",
    "║  Current Teaching Lab Configuration (hive-site.xml):          ║\n",
    "║                                                                ║\n",
    "║  <property>                                                    ║\n",
    "║      <name>hive.execution.engine</name>                        ║\n",
    "║      <value>mr</value>   ← Currently MapReduce                 ║\n",
    "║  </property>                                                   ║\n",
    "║                                                                ║\n",
    "║  To change to Spark, you would set:                            ║\n",
    "║      <value>spark</value>                                      ║\n",
    "╚════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hive on Spark Configuration\n",
    "\n",
    "To configure Hive to use Spark as its execution engine, you need:\n",
    "\n",
    "1. **Spark Assembly JAR** accessible to Hive\n",
    "2. **Hive configuration** updated\n",
    "3. **YARN resources** for Spark executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration required for Hive on Spark\n",
    "print(\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "CONFIGURATION REQUIRED FOR HIVE ON SPARK\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "1. hive-site.xml:\n",
    "   <property>\n",
    "       <name>hive.execution.engine</name>\n",
    "       <value>spark</value>\n",
    "   </property>\n",
    "   <property>\n",
    "       <name>spark.master</name>\n",
    "       <value>yarn</value>\n",
    "   </property>\n",
    "   <property>\n",
    "       <name>spark.submit.deployMode</name>\n",
    "       <value>client</value>\n",
    "   </property>\n",
    "\n",
    "2. Environment Variable:\n",
    "   export SPARK_HOME=/opt/spark\n",
    "   \n",
    "3. Hive needs Spark JARs:\n",
    "   - Copy spark-assembly JAR to Hive's lib directory\n",
    "   - Or set spark.home in hive-site.xml\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Better Approach - Use Spark SQL Directly\n",
    "\n",
    "Instead of configuring Hive to use Spark, the modern approach is:\n",
    "\n",
    "**Use Spark SQL directly with Hive Metastore!**\n",
    "\n",
    "This gives you:\n",
    "- Spark's fast in-memory processing\n",
    "- Access to all Hive tables via metastore\n",
    "- No complex Hive-on-Spark configuration\n",
    "- Better control over resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test database and table for comparison\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS engine_demo\")\n",
    "spark.sql(\"USE engine_demo\")\n",
    "\n",
    "# Generate sample data\n",
    "from pyspark.sql.functions import rand, randn, floor, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a larger dataset for meaningful comparison\n",
    "num_rows = 100000\n",
    "\n",
    "test_data = spark.range(num_rows) \\\n",
    "    .withColumn(\"category\", (floor(rand() * 10)).cast(\"int\")) \\\n",
    "    .withColumn(\"value\", (rand() * 1000)) \\\n",
    "    .withColumn(\"region\", (floor(rand() * 5)).cast(\"int\"))\n",
    "\n",
    "# Save as Hive table\n",
    "test_data.write.mode(\"overwrite\").saveAsTable(\"benchmark_data\")\n",
    "\n",
    "print(f\"Created benchmark table with {num_rows:,} rows\")\n",
    "spark.sql(\"SELECT COUNT(*) as row_count FROM benchmark_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Spark SQL performance\n",
    "import time\n",
    "\n",
    "# Complex aggregation query\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        region,\n",
    "        COUNT(*) as count,\n",
    "        SUM(value) as total_value,\n",
    "        AVG(value) as avg_value,\n",
    "        MAX(value) as max_value,\n",
    "        MIN(value) as min_value\n",
    "    FROM benchmark_data\n",
    "    GROUP BY category, region\n",
    "    ORDER BY total_value DESC\n",
    "\"\"\"\n",
    "\n",
    "# Time Spark SQL execution\n",
    "start = time.time()\n",
    "result = spark.sql(query)\n",
    "result.collect()  # Force execution\n",
    "spark_time = time.time() - start\n",
    "\n",
    "print(f\"=== Spark SQL Results ===\")\n",
    "result.show(10)\n",
    "print(f\"\\n⏱️  Spark SQL execution time: {spark_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show query execution plan\n",
    "print(\"=== Spark Query Execution Plan ===\")\n",
    "spark.sql(query).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: When to Use What\n",
    "\n",
    "### Use Hive with MapReduce when:\n",
    "- Large batch ETL jobs that run overnight\n",
    "- Memory is limited\n",
    "- Legacy systems that require Hive compatibility\n",
    "\n",
    "### Use Hive on Spark when:\n",
    "- You have existing Hive queries\n",
    "- Need faster performance without rewriting\n",
    "- Team is familiar with HiveQL\n",
    "\n",
    "### Use Spark SQL directly when:\n",
    "- Interactive analysis\n",
    "- Machine Learning pipelines\n",
    "- New development\n",
    "- Need DataFrame API flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison chart\n",
    "print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════╗\n",
    "║              EXECUTION ENGINE COMPARISON                              ║\n",
    "╠═══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                       ║\n",
    "║  Approach           │ Speed    │ Memory   │ Complexity │ Best For    ║\n",
    "║  ──────────────────────────────────────────────────────────────────  ║\n",
    "║  Hive + MR          │ Slow     │ Low      │ Simple     │ Batch ETL   ║\n",
    "║  Hive + Tez         │ Medium   │ Medium   │ Medium     │ General     ║\n",
    "║  Hive + Spark       │ Fast     │ High     │ Complex    │ Migration   ║\n",
    "║  Spark SQL Direct   │ Fastest  │ High     │ Simple     │ New Dev     ║\n",
    "║                                                                       ║\n",
    "╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Hybrid Architecture (Best Practice)\n",
    "\n",
    "In production, organizations often use:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    HIVE METASTORE                              │\n",
    "│               (Central Catalog)                                 │\n",
    "└────────────────────────┬────────────────────────────────────────┘\n",
    "                         │\n",
    "         ┌───────────────┼───────────────┐\n",
    "         │               │               │\n",
    "         ▼               ▼               ▼\n",
    "    ┌─────────┐    ┌─────────┐    ┌─────────────┐\n",
    "    │ HiveQL  │    │ Spark   │    │   Presto/   │\n",
    "    │ (Batch) │    │  SQL    │    │   Trino     │\n",
    "    │         │    │ (Fast)  │    │  (BI Tools) │\n",
    "    └─────────┘    └─────────┘    └─────────────┘\n",
    "         │               │               │\n",
    "         └───────────────┴───────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                    ┌─────────┐\n",
    "                    │   HDFS  │\n",
    "                    │  (ORC/  │\n",
    "                    │ Parquet)│\n",
    "                    └─────────┘\n",
    "```\n",
    "\n",
    "This allows:\n",
    "- **Batch ETL**: Hive with Tez or MR for scheduled jobs\n",
    "- **Interactive Analytics**: Spark SQL for data science\n",
    "- **BI Dashboards**: Presto/Trino for fast queries\n",
    "- **All sharing the same tables**: Via Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the shared metastore concept\n",
    "print(\"=== Tables accessible by ALL query engines ===\")\n",
    "spark.sql(\"SHOW TABLES IN engine_demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table is queryable via:\n",
    "print(\"\"\"\n",
    "The 'benchmark_data' table can be queried by:\n",
    "\n",
    "1. SPARK SQL (what we're using now):\n",
    "   spark.sql(\"SELECT * FROM engine_demo.benchmark_data\")\n",
    "\n",
    "2. HIVE (via beeline):\n",
    "   SELECT * FROM engine_demo.benchmark_data;\n",
    "\n",
    "3. PRESTO/TRINO (if configured):\n",
    "   SELECT * FROM hive.engine_demo.benchmark_data;\n",
    "\n",
    "4. JDBC/ODBC TOOLS:\n",
    "   Connect to HiveServer2 at port 10000\n",
    "\n",
    "All see the SAME data with the SAME schema!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Hive supports multiple execution engines**: MapReduce, Tez, Spark\n",
    "2. **Spark is fastest** but requires more memory\n",
    "3. **Modern approach**: Use Spark SQL directly with Hive Metastore\n",
    "4. **Hive Metastore is the key**: Central catalog for all tools\n",
    "5. **Hybrid architecture**: Different engines for different workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS benchmark_data\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS engine_demo CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

