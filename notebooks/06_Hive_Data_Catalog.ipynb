{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Apache Hive and the Data Catalog\n",
    "\n",
    "## Why Not Just Use CSV Files?\n",
    "\n",
    "In previous exercises, we read data directly from CSV files. While this works, it has significant limitations:\n",
    "\n",
    "| Approach | CSV Files | Hive Tables |\n",
    "|----------|-----------|-------------|\n",
    "| **Schema** | Inferred or defined in code | Stored centrally in catalog |\n",
    "| **Discovery** | Need to know file paths | Query catalog to find tables |\n",
    "| **Governance** | No access control | Role-based access possible |\n",
    "| **Evolution** | Breaking changes everywhere | Schema versioning supported |\n",
    "| **Optimization** | No statistics | Partition pruning, statistics |\n",
    "| **Sharing** | Copy files or share paths | Consistent view for all tools |\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the role of the Hive Metastore as a data catalog\n",
    "- Create managed and external tables with proper schemas\n",
    "- Use partitioning for efficient data organization\n",
    "- Compare governed tables vs raw file access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Hive Metastore\n",
    "\n",
    "The **Hive Metastore** is a central repository that stores:\n",
    "- **Database definitions** - logical groupings of tables\n",
    "- **Table schemas** - column names, types, and descriptions\n",
    "- **Partition information** - how data is organized\n",
    "- **Storage details** - file locations, formats, SerDe\n",
    "- **Statistics** - row counts, data sizes for optimization\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    HIVE METASTORE                          │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │\n",
    "│  │  Database   │  │   Tables    │  │ Partitions  │        │\n",
    "│  │ - sales_db  │──│ - orders    │──│ - year=2024 │        │\n",
    "│  │ - hr_db     │  │ - customers │  │ - month=01  │        │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────┘        │\n",
    "│         │                │                │                │\n",
    "│         ▼                ▼                ▼                │\n",
    "│  ┌─────────────────────────────────────────────┐          │\n",
    "│  │           PostgreSQL Backend               │          │\n",
    "│  │  (Persistent metadata storage)             │          │\n",
    "│  └─────────────────────────────────────────────┘          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "         │              │              │\n",
    "         ▼              ▼              ▼\n",
    "    ┌─────────┐   ┌─────────┐   ┌─────────┐\n",
    "    │  Spark  │   │  Hive   │   │ Presto  │\n",
    "    │  Query  │   │  Query  │   │ Query   │\n",
    "    └─────────┘   └─────────┘   └─────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session with Hive support created!\n",
      "Application ID: application_1768092706713_0009\n"
     ]
    }
   ],
   "source": [
    "# Connect to HiveServer2 using beeline (command-line interface)\n",
    "# In a real scenario, we'd use PyHive or Spark with Hive support\n",
    "\n",
    "# For this notebook, we'll demonstrate using Spark with Hive Metastore\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive Data Catalog Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session with Hive support created!\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploring the Catalog\n",
    "\n",
    "Let's see what's already in our metastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Databases in Catalog ===\n",
      "+---------------+\n",
      "|namespace      |\n",
      "+---------------+\n",
      "|default        |\n",
      "|sales_analytics|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all databases in the catalog\n",
    "print(\"=== Databases in Catalog ===\")\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tables in 'default' database ===\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List tables in the default database\n",
    "print(\"=== Tables in 'default' database ===\")\n",
    "spark.sql(\"SHOW TABLES IN default\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Governed Database\n",
    "\n",
    "Let's create a proper database for our sales data with governance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------------------------------------+\n",
      "|info_name     |info_value                                                 |\n",
      "+--------------+-----------------------------------------------------------+\n",
      "|Catalog Name  |spark_catalog                                              |\n",
      "|Namespace Name|sales_analytics                                            |\n",
      "|Comment       |Sales data warehouse for analytics team                    |\n",
      "|Location      |hdfs://namenode:9000/user/hive/warehouse/sales_analytics.db|\n",
      "|Owner         |jovyan                                                     |\n",
      "|Properties    |                                                           |\n",
      "+--------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new database with description and location\n",
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS sales_analytics\n",
    "    COMMENT 'Sales data warehouse for analytics team'\n",
    "    LOCATION '/user/hive/warehouse/sales_analytics.db'\n",
    "\"\"\")\n",
    "\n",
    "# Verify creation\n",
    "spark.sql(\"DESCRIBE DATABASE EXTENDED sales_analytics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using: sales_analytics database\n"
     ]
    }
   ],
   "source": [
    "# Switch to our new database\n",
    "spark.sql(\"USE sales_analytics\")\n",
    "print(\"Now using: sales_analytics database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Managed Tables vs External Tables\n",
    "\n",
    "### Managed Tables\n",
    "- Hive owns the data\n",
    "- DROP TABLE deletes the data\n",
    "- Best for ETL outputs and internal tables\n",
    "\n",
    "### External Tables  \n",
    "- Hive only manages metadata\n",
    "- DROP TABLE leaves data intact\n",
    "- Best for raw data landing zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Managed table 'customers' created!\n",
      "+----------------------------+---------------------------------------------------------------------+------------------------------+\n",
      "|col_name                    |data_type                                                            |comment                       |\n",
      "+----------------------------+---------------------------------------------------------------------+------------------------------+\n",
      "|customer_id                 |int                                                                  |Unique customer identifier    |\n",
      "|first_name                  |string                                                               |Customer first name           |\n",
      "|last_name                   |string                                                               |Customer last name            |\n",
      "|email                       |string                                                               |Contact email address         |\n",
      "|signup_date                 |date                                                                 |Date customer registered      |\n",
      "|loyalty_tier                |string                                                               |Bronze, Silver, Gold, Platinum|\n",
      "|                            |                                                                     |                              |\n",
      "|# Detailed Table Information|                                                                     |                              |\n",
      "|Catalog                     |spark_catalog                                                        |                              |\n",
      "|Database                    |sales_analytics                                                      |                              |\n",
      "|Table                       |customers                                                            |                              |\n",
      "|Owner                       |jovyan                                                               |                              |\n",
      "|Created Time                |Sun Jan 11 15:53:17 UTC 2026                                         |                              |\n",
      "|Last Access                 |UNKNOWN                                                              |                              |\n",
      "|Created By                  |Spark 3.5.0                                                          |                              |\n",
      "|Type                        |MANAGED                                                              |                              |\n",
      "|Provider                    |hive                                                                 |                              |\n",
      "|Comment                     |Master customer dimension table                                      |                              |\n",
      "|Table Properties            |[transient_lastDdlTime=1768146797]                                   |                              |\n",
      "|Location                    |hdfs://namenode:9000/user/hive/warehouse/sales_analytics.db/customers|                              |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe          |                              |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat        |                              |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat       |                              |\n",
      "|Storage Properties          |[serialization.format=1]                                             |                              |\n",
      "|Partition Provider          |Catalog                                                              |                              |\n",
      "+----------------------------+---------------------------------------------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a MANAGED table with explicit schema\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customers (\n",
    "        customer_id INT COMMENT 'Unique customer identifier',\n",
    "        first_name STRING COMMENT 'Customer first name',\n",
    "        last_name STRING COMMENT 'Customer last name',\n",
    "        email STRING COMMENT 'Contact email address',\n",
    "        signup_date DATE COMMENT 'Date customer registered',\n",
    "        loyalty_tier STRING COMMENT 'Bronze, Silver, Gold, Platinum'\n",
    "    )\n",
    "    COMMENT 'Master customer dimension table'\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "print(\"Managed table 'customers' created!\")\n",
    "spark.sql(\"DESCRIBE EXTENDED customers\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External table 'raw_transactions' created!\n"
     ]
    }
   ],
   "source": [
    "# Create an EXTERNAL table pointing to existing data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS raw_transactions (\n",
    "        transaction_id STRING,\n",
    "        customer_id INT,\n",
    "        product_id STRING,\n",
    "        quantity INT,\n",
    "        unit_price DECIMAL(10,2),\n",
    "        transaction_date TIMESTAMP\n",
    "    )\n",
    "    COMMENT 'Raw transaction data - external source'\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "    LOCATION '/user/student/data/transactions'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\")\n",
    "\n",
    "print(\"External table 'raw_transactions' created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Partitioned Tables for Performance\n",
    "\n",
    "Partitioning organizes data into directories, enabling **partition pruning**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned table 'orders' created!\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                         |comment|\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|order_id                    |string                                                            |NULL   |\n",
      "|customer_id                 |int                                                               |NULL   |\n",
      "|order_total                 |decimal(12,2)                                                     |NULL   |\n",
      "|order_status                |string                                                            |NULL   |\n",
      "|created_at                  |timestamp                                                         |NULL   |\n",
      "|order_year                  |int                                                               |NULL   |\n",
      "|order_month                 |int                                                               |NULL   |\n",
      "|# Partition Information     |                                                                  |       |\n",
      "|# col_name                  |data_type                                                         |comment|\n",
      "|order_year                  |int                                                               |NULL   |\n",
      "|order_month                 |int                                                               |NULL   |\n",
      "|                            |                                                                  |       |\n",
      "|# Detailed Table Information|                                                                  |       |\n",
      "|Catalog                     |spark_catalog                                                     |       |\n",
      "|Database                    |sales_analytics                                                   |       |\n",
      "|Table                       |orders                                                            |       |\n",
      "|Owner                       |jovyan                                                            |       |\n",
      "|Created Time                |Sun Jan 11 18:44:26 UTC 2026                                      |       |\n",
      "|Last Access                 |UNKNOWN                                                           |       |\n",
      "|Created By                  |Spark 3.5.0                                                       |       |\n",
      "|Type                        |MANAGED                                                           |       |\n",
      "|Provider                    |hive                                                              |       |\n",
      "|Comment                     |Order fact table partitioned by date                              |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1768157066]                                |       |\n",
      "|Location                    |hdfs://namenode:9000/user/hive/warehouse/sales_analytics.db/orders|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe       |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat     |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat    |       |\n",
      "|Storage Properties          |[serialization.format=1]                                          |       |\n",
      "|Partition Provider          |Catalog                                                           |       |\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a partitioned fact table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id STRING,\n",
    "        customer_id INT,\n",
    "        order_total DECIMAL(12,2),\n",
    "        order_status STRING,\n",
    "        created_at TIMESTAMP\n",
    "    )\n",
    "    COMMENT 'Order fact table partitioned by date'\n",
    "    PARTITIONED BY (order_year INT, order_month INT)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "print(\"Partitioned table 'orders' created!\")\n",
    "spark.sql(\"DESCRIBE EXTENDED orders\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data inserted into partitioned table!\n"
     ]
    }
   ],
   "source": [
    "# Insert sample data into partitions\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# Create sample order data\n",
    "sample_orders = [\n",
    "    (\"ORD-001\", 101, 150.00, \"completed\", datetime(2024, 1, 15, 10, 30)),\n",
    "    (\"ORD-002\", 102, 275.50, \"completed\", datetime(2024, 1, 20, 14, 45)),\n",
    "    (\"ORD-003\", 101, 89.99, \"shipped\", datetime(2024, 2, 5, 9, 15)),\n",
    "    (\"ORD-004\", 103, 420.00, \"pending\", datetime(2024, 2, 10, 16, 20)),\n",
    "    (\"ORD-004\", 103, 420.00, \"pending\", datetime(2025, 2, 10, 16, 20)),\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    sample_orders, \n",
    "    [\"order_id\", \"customer_id\", \"order_total\", \"order_status\", \"created_at\"]\n",
    ")\n",
    "\n",
    "# Add partition columns\n",
    "from pyspark.sql.functions import year, month\n",
    "orders_df = orders_df.withColumn(\"order_year\", year(\"created_at\")) \\\n",
    "                     .withColumn(\"order_month\", month(\"created_at\"))\n",
    "\n",
    "# Write to Hive table with partitioning\n",
    "orders_df.write.mode(\"append\").insertInto(\"orders\")\n",
    "\n",
    "print(\"Sample data inserted into partitioned table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Table Partitions ===\n",
      "+-----------------------------+\n",
      "|partition                    |\n",
      "+-----------------------------+\n",
      "|order_year=2024/order_month=1|\n",
      "|order_year=2024/order_month=2|\n",
      "|order_year=2025/order_month=2|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the partitions\n",
    "print(\"=== Table Partitions ===\")\n",
    "spark.sql(\"SHOW PARTITIONS orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query January 2024 Orders (Partition Pruning) ===\n",
      "+--------+-----------+-----------+------------+\n",
      "|order_id|customer_id|order_total|order_status|\n",
      "+--------+-----------+-----------+------------+\n",
      "| ORD-001|        101|     150.00|   completed|\n",
      "| ORD-002|        102|     275.50|   completed|\n",
      "| ORD-001|        101|     150.00|   completed|\n",
      "| ORD-002|        102|     275.50|   completed|\n",
      "+--------+-----------+-----------+------------+\n",
      "\n",
      "\n",
      "=== Query Plan (Notice Partition Filters) ===\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\\n* ColumnarToRow (2)\\n+- Scan parquet spark_catalog.sales_analytics.orders (1)\\n\\n\\n(1) Scan parquet spark_catalog.sales_analytics.orders\\nOutput [7]: [order_id#482, customer_id#483, order_total#484, order_status#485, created_at#486, order_year#487, order_month#488]\\nBatched: true\\nLocation: InMemoryFileIndex [hdfs://namenode:9000/user/hive/warehouse/sales_analytics.db/orders/order_year=2024/order_month=1]\\nPartitionFilters: [isnotnull(order_year#487), isnotnull(order_month#488), (order_year#487 = 2024), (order_month#488 = 1)]\\nReadSchema: struct<order_id:string,customer_id:int,order_total:decimal(12,2),order_status:string,created_at:timestamp>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [7]: [order_id#482, customer_id#483, order_total#484, order_status#485, created_at#486, order_year#487, order_month#488]\\n\\n|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with partition pruning\n",
    "print(\"=== Query January 2024 Orders (Partition Pruning) ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, customer_id, order_total, order_status\n",
    "    FROM orders\n",
    "    WHERE order_year = 2024 AND order_month = 1\n",
    "\"\"\").show()\n",
    "\n",
    "# Explain the query to see partition pruning\n",
    "print(\"\\n=== Query Plan (Notice Partition Filters) ===\")\n",
    "spark.sql(\"\"\"\n",
    "    EXPLAIN FORMATTED\n",
    "    SELECT * FROM orders WHERE order_year = 2024 AND order_month = 1\n",
    "\"\"\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Schema Discovery and Documentation\n",
    "\n",
    "Unlike raw files, catalog tables are self-documenting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tables in sales_analytics ===\n",
      "+---------------+----------------+-----------+\n",
      "|namespace      |tableName       |isTemporary|\n",
      "+---------------+----------------+-----------+\n",
      "|sales_analytics|customers       |false      |\n",
      "|sales_analytics|orders          |false      |\n",
      "|sales_analytics|raw_transactions|false      |\n",
      "+---------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all tables in our database\n",
    "print(\"=== Tables in sales_analytics ===\")\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== customers Table Details ===\n",
      "+----------------------------+---------------------------------------------------------------------+------------------------------+\n",
      "|col_name                    |data_type                                                            |comment                       |\n",
      "+----------------------------+---------------------------------------------------------------------+------------------------------+\n",
      "|customer_id                 |int                                                                  |Unique customer identifier    |\n",
      "|first_name                  |string                                                               |Customer first name           |\n",
      "|last_name                   |string                                                               |Customer last name            |\n",
      "|email                       |string                                                               |Contact email address         |\n",
      "|signup_date                 |date                                                                 |Date customer registered      |\n",
      "|loyalty_tier                |string                                                               |Bronze, Silver, Gold, Platinum|\n",
      "|                            |                                                                     |                              |\n",
      "|# Detailed Table Information|                                                                     |                              |\n",
      "|Catalog                     |spark_catalog                                                        |                              |\n",
      "|Database                    |sales_analytics                                                      |                              |\n",
      "|Table                       |customers                                                            |                              |\n",
      "|Owner                       |jovyan                                                               |                              |\n",
      "|Created Time                |Sun Jan 11 15:53:17 UTC 2026                                         |                              |\n",
      "|Last Access                 |UNKNOWN                                                              |                              |\n",
      "|Created By                  |Spark 3.5.0                                                          |                              |\n",
      "|Type                        |MANAGED                                                              |                              |\n",
      "|Provider                    |hive                                                                 |                              |\n",
      "|Comment                     |Master customer dimension table                                      |                              |\n",
      "|Table Properties            |[transient_lastDdlTime=1768146797]                                   |                              |\n",
      "|Location                    |hdfs://namenode:9000/user/hive/warehouse/sales_analytics.db/customers|                              |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe          |                              |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat        |                              |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat       |                              |\n",
      "|Storage Properties          |[serialization.format=1]                                             |                              |\n",
      "|Partition Provider          |Catalog                                                              |                              |\n",
      "+----------------------------+---------------------------------------------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get detailed table information\n",
    "print(\"=== customers Table Details ===\")\n",
    "spark.sql(\"DESCRIBE FORMATTED customers\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== How to recreate the table ===\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE sales_analytics.orders (\\n  order_id STRING,\\n  customer_id INT,\\n  order_total DECIMAL(12,2),\\n  order_status STRING,\\n  created_at TIMESTAMP,\\n  order_year INT,\\n  order_month INT)\\nUSING parquet\\nPARTITIONED BY (order_year, order_month)\\nCOMMENT 'Order fact table partitioned by date'\\nTBLPROPERTIES (\\n  'transient_lastDdlTime' = '1768157066')\\n|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# View table creation DDL\n",
    "print(\"=== How to recreate the table ===\")\n",
    "spark.sql(\"SHOW CREATE TABLE orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: The Problem with Raw Files\n",
    "\n",
    "Let's contrast governed tables with raw file access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The OLD WAY: Reading a CSV file directly\n",
    "# Problems:\n",
    "# 1. Need to know exact file path\n",
    "# 2. Schema must be defined in every script\n",
    "# 3. No documentation or discovery\n",
    "# 4. No access control\n",
    "\n",
    "print(\"=== THE OLD WAY (Raw File Access) ===\")\n",
    "print(\"\"\"\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\\  # <- Slow, unreliable\n",
    "    .csv(\"/some/path/to/transactions.csv\")  # <- Hardcoded path!\n",
    "    \n",
    "# Every user needs to:\n",
    "# - Know the file location\n",
    "# - Know the schema\n",
    "# - Handle schema changes manually\n",
    "# - No data lineage\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THE NEW WAY (Catalog Access) ===\n",
      "\n",
      "df = spark.table(\"sales_analytics.orders\")\n",
      "\n",
      "# Benefits:\n",
      "# ✓ Schema is predefined and documented\n",
      "# ✓ Location abstracted away\n",
      "# ✓ Access control via database permissions\n",
      "# ✓ Statistics enable query optimization\n",
      "# ✓ All tools see the same data definition\n",
      "\n",
      "\n",
      "Querying orders table from catalog:\n",
      "+--------+-----------+-----------+------------+-------------------+----------+-----------+\n",
      "|order_id|customer_id|order_total|order_status|         created_at|order_year|order_month|\n",
      "+--------+-----------+-----------+------------+-------------------+----------+-----------+\n",
      "| ORD-004|        103|     420.00|     pending|2025-02-10 16:20:00|      2025|          2|\n",
      "| ORD-001|        101|     150.00|   completed|2024-01-15 10:30:00|      2024|          1|\n",
      "| ORD-002|        102|     275.50|   completed|2024-01-20 14:45:00|      2024|          1|\n",
      "| ORD-001|        101|     150.00|   completed|2024-01-15 10:30:00|      2024|          1|\n",
      "| ORD-002|        102|     275.50|   completed|2024-01-20 14:45:00|      2024|          1|\n",
      "| ORD-003|        101|      89.99|     shipped|2024-02-05 09:15:00|      2024|          2|\n",
      "| ORD-004|        103|     420.00|     pending|2024-02-10 16:20:00|      2024|          2|\n",
      "| ORD-003|        101|      89.99|     shipped|2024-02-05 09:15:00|      2024|          2|\n",
      "| ORD-004|        103|     420.00|     pending|2024-02-10 16:20:00|      2024|          2|\n",
      "+--------+-----------+-----------+------------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The NEW WAY: Query from catalog\n",
    "print(\"=== THE NEW WAY (Catalog Access) ===\")\n",
    "print(\"\"\"\n",
    "df = spark.table(\"sales_analytics.orders\")\n",
    "\n",
    "# Benefits:\n",
    "# ✓ Schema is predefined and documented\n",
    "# ✓ Location abstracted away\n",
    "# ✓ Access control via database permissions\n",
    "# ✓ Statistics enable query optimization\n",
    "# ✓ All tools see the same data definition\n",
    "\"\"\")\n",
    "\n",
    "# Actually query the table\n",
    "df = spark.table(\"sales_analytics.orders\")\n",
    "print(\"\\nQuerying orders table from catalog:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Table Statistics for Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute table statistics\n",
    "spark.sql(\"ANALYZE TABLE orders COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE orders COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "\n",
    "print(\"Statistics computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the statistics\n",
    "print(\"=== Table Statistics ===\")\n",
    "spark.sql(\"DESCRIBE EXTENDED orders\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Create a Product Catalog Table\n",
    "Create a managed table called `products` with columns for product_id, name, category, price, and inventory_count.\n",
    "\n",
    "### Exercise 2: Create a Partitioned Sales Table\n",
    "Create an external table for sales data partitioned by region and year.\n",
    "\n",
    "### Exercise 3: Compare Query Plans\n",
    "Write a query against the partitioned table and examine the query plan to verify partition pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# spark.sql(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS products (\n",
    "#         ...\n",
    "#     )\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS customers\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS raw_transactions\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS orders\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS sales_analytics CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
