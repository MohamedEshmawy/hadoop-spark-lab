{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: HDFS Basics\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand HDFS architecture (NameNode, DataNodes, blocks)\n",
    "- Perform file operations: upload, list, download, delete\n",
    "- Explore replication factor and block concepts\n",
    "- Use the HDFS Web UI to visualize the filesystem\n",
    "\n",
    "## Prerequisites\n",
    "- Cluster is running (`./scripts/start-lab.sh`)\n",
    "- Sanity checks passed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploring HDFS Commands\n",
    "\n",
    "HDFS commands are similar to Linux filesystem commands, but prefixed with `hdfs dfs -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the root directory of HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory in HDFS for our exercises\n",
    "!hdfs dfs -mkdir -p /user/student/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the directory was created\n",
    "!hdfs dfs -ls /user/student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Uploading Files to HDFS\n",
    "\n",
    "Let's upload our sample data to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files are available locally\n",
    "!ls -la /home/jovyan/data/sales/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the transactions file to HDFS\n",
    "!hdfs dfs -put /home/jovyan/data/sales/transactions.csv /user/student/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload products catalog\n",
    "!hdfs dfs -put /home/jovyan/data/products/catalog.csv /user/student/data/\n",
    "!hdfs dfs -put /home/jovyan/data/products/catalog.json /user/student/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploads\n",
    "!hdfs dfs -ls -h /user/student/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Blocks and Replication\n",
    "\n",
    "HDFS splits large files into blocks (default 128MB in production, 16MB in our lab).\n",
    "Each block is replicated across multiple DataNodes for fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check block information for our file\n",
    "!hdfs fsck /user/student/data/transactions.csv -files -blocks -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current replication factor\n",
    "!hdfs dfs -stat '%r' /user/student/data/transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change replication factor to 3\n",
    "!hdfs dfs -setrep 3 /user/student/data/transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new replication factor\n",
    "!hdfs fsck /user/student/data/transactions.csv -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checkpoint Question 1\n",
    "Open the HDFS NameNode UI at http://localhost:9870 and navigate to:\n",
    "**Utilities ‚Üí Browse the file system**\n",
    "\n",
    "Find the `transactions.csv` file. \n",
    "- How many blocks does it have?\n",
    "- What is the block size?\n",
    "- On which DataNodes are the blocks stored?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "(Write your observations here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Reading Files from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few lines\n",
    "!hdfs dfs -head /user/student/data/transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total lines\n",
    "!hdfs dfs -cat /user/student/data/transactions.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file statistics\n",
    "!hdfs dfs -du -h /user/student/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cluster Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DataNode status\n",
    "!hdfs dfsadmin -report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

