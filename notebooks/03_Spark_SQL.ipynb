{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Spark SQL\n",
    "\n",
    "## Learning Objectives\n",
    "- Load data from CSV, JSON, and Parquet formats\n",
    "- Execute SQL queries on DataFrames\n",
    "- Join datasets from different sources\n",
    "- Write results back to HDFS in optimized formats\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sum, avg, count, desc\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark SQL session ready: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data from Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transactions from CSV\n",
    "transactions = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "print(f\"Transactions: {transactions.count()} rows\")\n",
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load products from JSON\n",
    "products = spark.read.json(\"hdfs:///user/student/data/catalog.json\")\n",
    "\n",
    "print(f\"Products: {products.count()} rows\")\n",
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary views for SQL\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "products.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregation query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        store_region,\n",
    "        COUNT(*) as num_transactions,\n",
    "        ROUND(SUM(total_amount), 2) as total_sales,\n",
    "        ROUND(AVG(total_amount), 2) as avg_sale\n",
    "    FROM transactions\n",
    "    GROUP BY store_region\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based analysis\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(TO_DATE(transaction_date, 'yyyy-MM-dd')) as year,\n",
    "        MONTH(TO_DATE(transaction_date, 'yyyy-MM-dd')) as month,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(total_amount), 2) as revenue\n",
    "    FROM transactions\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year, month\n",
    "\"\"\").show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment method analysis\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        payment_method,\n",
    "        is_online,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(total_amount), 2) as avg_amount\n",
    "    FROM transactions\n",
    "    GROUP BY payment_method, is_online\n",
    "    ORDER BY payment_method, is_online\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Joining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join transactions with products\n",
    "sales_with_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.transaction_date,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        t.quantity,\n",
    "        t.unit_price,\n",
    "        t.total_amount,\n",
    "        p.cost_price * t.quantity as total_cost,\n",
    "        t.total_amount - (p.cost_price * t.quantity) as profit\n",
    "    FROM transactions t\n",
    "    JOIN products p ON t.product_id = p.product_id\n",
    "\"\"\")\n",
    "\n",
    "sales_with_products.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit by category\n",
    "sales_with_products.createOrReplaceTempView(\"sales_detail\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(total_amount), 2) as revenue,\n",
    "        ROUND(SUM(profit), 2) as total_profit,\n",
    "        ROUND(AVG(profit), 2) as avg_profit_per_sale\n",
    "    FROM sales_detail\n",
    "    GROUP BY category\n",
    "    ORDER BY total_profit DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checkpoint Question 3\n",
    "Look at the Spark UI (SQL tab). Find the join query.\n",
    "- What type of join was used (broadcast, sort-merge, shuffle-hash)?\n",
    "- Why do you think Spark chose that strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Writing Data in Optimized Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet (columnar, compressed)\n",
    "sales_with_products.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"hdfs:///user/student/output/sales_parquet\")\n",
    "\n",
    "print(\"Written to Parquet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned by category (useful for filtering)\n",
    "sales_with_products.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(\"hdfs:///user/student/output/sales_by_category\")\n",
    "\n",
    "print(\"Written with partitioning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output\n",
    "!hdfs dfs -ls /user/student/output/sales_by_category/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back and verify\n",
    "parquet_data = spark.read.parquet(\"hdfs:///user/student/output/sales_parquet\")\n",
    "print(f\"Read back {parquet_data.count()} rows from Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

