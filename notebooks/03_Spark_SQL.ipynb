{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Spark SQL\n",
    "\n",
    "## Learning Objectives\n",
    "- Load data from CSV, JSON, and Parquet formats\n",
    "- Execute SQL queries on DataFrames\n",
    "- Join datasets from different sources\n",
    "- Write results back to HDFS in optimized formats\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark SQL session ready: application_1768092706713_0008\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sum, avg, count, desc\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark SQL session ready: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxPartitionBytes  = 128.0 MB\n",
      "defaultParallelism = 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"maxPartitionBytes  = {int(spark.conf.get('spark.sql.files.maxPartitionBytes')[:-1])/(1024*1024)} MB\")\n",
    "print(\"defaultParallelism =\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data from Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions: 100000 rows\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_time: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- store_region: string (nullable = true)\n",
      " |-- is_online: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load transactions from CSV\n",
    "transactions = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "print(f\"Transactions: {transactions.count()} rows\")\n",
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 2\n",
      "['hdfs://namenode:9000/user/student/data/transactions.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitions:\", transactions.rdd.getNumPartitions())\n",
    "print(transactions.inputFiles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: 500 rows\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- cost_price: double (nullable = true)\n",
      " |-- in_stock: boolean (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- supplier_id: string (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- weight_kg: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load products from JSON\n",
    "products = spark.read.json(\"hdfs:///user/student/data/catalog.json\")\n",
    "\n",
    "print(f\"Products: {products.count()} rows\")\n",
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary views for SQL\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "products.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------+--------+\n",
      "|store_region|num_transactions|  total_sales|avg_sale|\n",
      "+------------+----------------+-------------+--------+\n",
      "|        West|           21575|2.831221976E7| 1312.27|\n",
      "|       North|           20048|2.652123459E7| 1322.89|\n",
      "|       South|           20191|2.649756461E7| 1312.35|\n",
      "|     Central|           19364|2.520488752E7| 1301.64|\n",
      "|        East|           18822|2.438263004E7| 1295.43|\n",
      "+------------+----------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic aggregation query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        store_region,\n",
    "        COUNT(*) as num_transactions,\n",
    "        ROUND(SUM(total_amount), 2) as total_sales,\n",
    "        ROUND(AVG(total_amount), 2) as avg_sale\n",
    "    FROM transactions\n",
    "    GROUP BY store_region\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+----------+\n",
      "|year|month|transactions|   revenue|\n",
      "+----+-----+------------+----------+\n",
      "|2023|    1|        4259|5565325.87|\n",
      "|2023|    2|        3776|4997062.64|\n",
      "|2023|    3|        4189| 5416674.4|\n",
      "|2023|    4|        3989|5202017.56|\n",
      "|2023|    5|        4189|5449502.91|\n",
      "|2023|    6|        4060|5405176.48|\n",
      "|2023|    7|        4224|5491132.37|\n",
      "|2023|    8|        4295|5545680.62|\n",
      "|2023|    9|        4113|5480710.13|\n",
      "|2023|   10|        4299|5666773.14|\n",
      "|2023|   11|        4034|5216630.28|\n",
      "|2023|   12|        4340|5689366.69|\n",
      "|2024|    1|        4330|5594096.35|\n",
      "|2024|    2|        3978|5238518.21|\n",
      "|2024|    3|        4220|5654334.45|\n",
      "|2024|    4|        4159|5423307.73|\n",
      "|2024|    5|        4158| 5442786.8|\n",
      "|2024|    6|        4117| 5416887.4|\n",
      "|2024|    7|        4304|5548275.61|\n",
      "|2024|    8|        4257|5466102.23|\n",
      "|2024|    9|        4184|5493626.82|\n",
      "|2024|   10|        4225|5579579.63|\n",
      "|2024|   11|        4015|5349776.86|\n",
      "|2024|   12|        4286|5585191.34|\n",
      "+----+-----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time-based analysis\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(TO_DATE(transaction_date, 'yyyy-MM-dd')) as year,\n",
    "        MONTH(TO_DATE(transaction_date, 'yyyy-MM-dd')) as month,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(total_amount), 2) as revenue\n",
    "    FROM transactions\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year, month\n",
    "\"\"\").show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----+----------+\n",
      "|payment_method|is_online|count|avg_amount|\n",
      "+--------------+---------+-----+----------+\n",
      "|          Cash|    false|10025|   1299.04|\n",
      "|          Cash|     true|10114|   1308.34|\n",
      "|   Credit Card|    false|10009|   1309.59|\n",
      "|   Credit Card|     true|10035|   1320.26|\n",
      "|    Debit Card|    false| 9847|   1302.93|\n",
      "|    Debit Card|     true| 9886|   1304.85|\n",
      "|     Gift Card|    false| 9889|   1316.72|\n",
      "|     Gift Card|     true|10240|   1322.57|\n",
      "|        PayPal|    false|10002|    1293.2|\n",
      "|        PayPal|     true| 9953|    1314.0|\n",
      "+--------------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Payment method analysis\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        payment_method,\n",
    "        is_online,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(total_amount), 2) as avg_amount\n",
    "    FROM transactions\n",
    "    GROUP BY payment_method, is_online\n",
    "    ORDER BY payment_method, is_online\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Joining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+---------------+--------+----------+------------+------------------+------------------+\n",
      "|transaction_id|transaction_date|        product_name|       category|quantity|unit_price|total_amount|        total_cost|            profit|\n",
      "+--------------+----------------+--------------------+---------------+--------+----------+------------+------------------+------------------+\n",
      "|   TXN00000001|      2024-06-03|Home & Garden Ite...|  Home & Garden|       1|    259.06|      259.06|            139.12|            119.94|\n",
      "|   TXN00000002|      2023-11-22|     Office Item 288|         Office|       6|     442.7|      2656.2|1114.8600000000001|1541.3399999999997|\n",
      "|   TXN00000003|      2023-08-13|Health & Beauty I...|Health & Beauty|       1|     116.0|       116.0|             50.17|             65.83|\n",
      "|   TXN00000004|      2023-06-20|Electronics Item 230|    Electronics|       4|    187.53|      750.12|            364.64|            385.48|\n",
      "|   TXN00000005|      2024-01-24|Food & Beverage I...|Food & Beverage|       9|      5.14|       46.28|             35.64|             10.64|\n",
      "|   TXN00000006|      2023-08-04|Electronics Item 393|    Electronics|       9|     36.53|      328.74|            237.15|             91.59|\n",
      "|   TXN00000007|      2023-06-15| Automotive Item 441|     Automotive|       4|    197.29|      789.16|            456.88|            332.28|\n",
      "|   TXN00000008|      2024-06-23|Health & Beauty I...|Health & Beauty|       1|    477.45|      477.45|            301.33|            176.12|\n",
      "|   TXN00000009|      2024-12-05|  Automotive Item 81|     Automotive|       5|    193.65|      968.26|505.34999999999997|            462.91|\n",
      "|   TXN00000010|      2023-10-04|Health & Beauty I...|Health & Beauty|       3|     97.22|      291.66|            182.22|109.44000000000003|\n",
      "+--------------+----------------+--------------------+---------------+--------+----------+------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Join transactions with products\n",
    "sales_with_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.transaction_date,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        t.quantity,\n",
    "        t.unit_price,\n",
    "        t.total_amount,\n",
    "        p.cost_price * t.quantity as total_cost,\n",
    "        t.total_amount - (p.cost_price * t.quantity) as profit\n",
    "    FROM transactions t\n",
    "    JOIN products p ON t.product_id = p.product_id\n",
    "\"\"\")\n",
    "\n",
    "sales_with_products.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-------------+------------+-------------------+\n",
      "|       category|transactions|      revenue|total_profit|avg_profit_per_sale|\n",
      "+---------------+------------+-------------+------------+-------------------+\n",
      "|         Office|       11109|1.663535544E7|   7347119.5|             661.37|\n",
      "|       Clothing|       11712|1.585799722E7|  6517862.36|             556.51|\n",
      "|         Sports|       11340|1.409068302E7|  6133215.62|             540.85|\n",
      "|     Automotive|       10433|1.335601151E7|  6048683.21|             579.76|\n",
      "|Food & Beverage|        9554|1.285170106E7|  5626330.56|              588.9|\n",
      "|           Toys|        8827| 1.20920478E7|  5399772.47|             611.73|\n",
      "|Health & Beauty|       10497|1.270791491E7|  5388642.39|             513.35|\n",
      "|  Home & Garden|       10196|1.318413938E7|  5379350.34|             527.59|\n",
      "|          Books|        9101|1.147227731E7|  4686487.61|             514.94|\n",
      "|    Electronics|        7231|   8670408.87|  3607807.03|             498.94|\n",
      "+---------------+------------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profit by category\n",
    "sales_with_products.createOrReplaceTempView(\"sales_detail\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(total_amount), 2) as revenue,\n",
    "        ROUND(SUM(profit), 2) as total_profit,\n",
    "        ROUND(AVG(profit), 2) as avg_profit_per_sale\n",
    "    FROM sales_detail\n",
    "    GROUP BY category\n",
    "    ORDER BY total_profit DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Writing Data in Optimized Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to Parquet!\n"
     ]
    }
   ],
   "source": [
    "# Write to Parquet (columnar, compressed)\n",
    "sales_with_products.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"hdfs:///user/student/output/sales_parquet\")\n",
    "\n",
    "print(\"Written to Parquet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written with partitioning!\n"
     ]
    }
   ],
   "source": [
    "# Write partitioned by category (useful for filtering)\n",
    "sales_with_products.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(\"hdfs:///user/student/output/sales_by_category\")\n",
    "\n",
    "print(\"Written with partitioning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "-rw-r--r--   3 jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/_SUCCESS\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Automotive\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Books\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Clothing\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Electronics\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Food & Beverage\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Health & Beauty\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Home & Garden\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Office\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Sports\n",
      "drwxr-xr-x   - jovyan supergroup          0 2026-01-11 13:52 /user/student/output/sales_by_category/category=Toys\n"
     ]
    }
   ],
   "source": [
    "# Check the output\n",
    "!hdfs dfs -ls /user/student/output/sales_by_category/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read back 100000 rows from Parquet\n"
     ]
    }
   ],
   "source": [
    "# Read back and verify\n",
    "parquet_data = spark.read.parquet(\"hdfs:///user/student/output/sales_parquet\")\n",
    "print(f\"Read back {parquet_data.count()} rows from Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
