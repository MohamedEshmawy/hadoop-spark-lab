{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Distributed Execution Concepts\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand partitioning and data distribution\n",
    "- Analyze shuffle operations and their costs\n",
    "- Read and interpret the Spark UI (stages, tasks, DAG)\n",
    "- Optimize jobs by understanding execution plans\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Distributed Execution Lab\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Application: {spark.sparkContext.applicationId}\")\n",
    "print(\"Open Spark UI at http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and check partitions\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs:///user/student/data/transactions.csv\")\n",
    "\n",
    "print(f\"Default partitions: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition to control parallelism\n",
    "df_4_parts = df.repartition(4)\n",
    "df_8_parts = df.repartition(8)\n",
    "\n",
    "print(f\"4 partitions: {df_4_parts.rdd.getNumPartitions()}\")\n",
    "print(f\"8 partitions: {df_8_parts.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See data distribution across partitions\n",
    "def show_partition_sizes(df, name):\n",
    "    sizes = df.rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).collect()\n",
    "    print(f\"{name}: {sizes} (total: {sum(sizes)})\")\n",
    "\n",
    "show_partition_sizes(df_4_parts, \"4 partitions\")\n",
    "show_partition_sizes(df_8_parts, \"8 partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Analyzing Stages and Shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple aggregation (causes shuffle)\n",
    "start = time.time()\n",
    "\n",
    "result1 = df.groupBy(\"store_region\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"), sum(\"total_amount\").alias(\"total\")) \\\n",
    "    .collect()\n",
    "\n",
    "print(f\"Aggregation took: {time.time() - start:.2f} seconds\")\n",
    "print(\"Check Spark UI -> Stages tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View execution plan\n",
    "df.groupBy(\"store_region\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Checkpoint Question 4\n",
    "Look at the Spark UI Stages tab:\n",
    "- How many stages were created for the aggregation?\n",
    "- Find the \"Shuffle Read\" and \"Shuffle Write\" metrics. What do they tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Join Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load products (small table)\n",
    "products = spark.read.json(\"hdfs:///user/student/data/catalog.json\")\n",
    "print(f\"Products size: {products.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular join (Spark will choose broadcast for small table)\n",
    "joined = df.join(products, \"product_id\")\n",
    "joined.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force broadcast hint\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined_broadcast = df.join(broadcast(products), \"product_id\")\n",
    "joined_broadcast.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and measure\n",
    "start = time.time()\n",
    "count = joined_broadcast.count()\n",
    "print(f\"Join result: {count} rows in {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: DAG Visualization\n",
    "\n",
    "Complex query with multiple stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query: Join, filter, aggregate, sort\n",
    "complex_result = df.join(products, \"product_id\") \\\n",
    "    .filter(col(\"is_online\") == True) \\\n",
    "    .groupBy(\"category\", \"store_region\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transactions\"),\n",
    "        sum(\"total_amount\").alias(\"revenue\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"revenue\"))\n",
    "\n",
    "# View the DAG\n",
    "complex_result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute\n",
    "complex_result.show(10)\n",
    "print(\"Check Spark UI -> SQL tab for visual DAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Caching for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache frequently used data\n",
    "df.cache()\n",
    "df.count()  # Materialize cache\n",
    "\n",
    "print(\"Data cached! Check Storage tab in Spark UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple queries on cached data\n",
    "start = time.time()\n",
    "for region in ['North', 'South', 'East', 'West', 'Central']:\n",
    "    count = df.filter(col(\"store_region\") == region).count()\n",
    "    print(f\"{region}: {count}\")\n",
    "print(f\"Total time: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist to free memory\n",
    "df.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
