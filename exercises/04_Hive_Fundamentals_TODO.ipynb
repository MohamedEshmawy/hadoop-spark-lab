{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Apache Hive Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Hive architecture and its role in the Hadoop ecosystem\n",
    "- Create databases, tables (managed and external)\n",
    "- Load data into Hive tables\n",
    "- Work with partitions and bucketing\n",
    "- Write HiveQL queries (SELECT, JOIN, GROUP BY, etc.)\n",
    "- Use Hive with Spark (SparkSQL + Hive Metastore)\n",
    "\n",
    "## Prerequisites\n",
    "- Completed HDFS and Spark exercises\n",
    "- Hive services running (metastore and hiveserver2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Connecting to Hive\n",
    "\n",
    "### Exercise 1.1: Connect via Beeline\n",
    "\n",
    "Open a terminal and connect to HiveServer2 using Beeline:\n",
    "\n",
    "```bash\n",
    "docker exec -it hiveserver2 beeline -u \"jdbc:hive2://localhost:10000/default\"\n",
    "```\n",
    "\n",
    "Once connected, run:\n",
    "```sql\n",
    "SHOW DATABASES;\n",
    "```\n",
    "\n",
    "**Question:** What databases exist by default?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Connect via PySpark with Hive Support\n",
    "\n",
    "Spark can use Hive Metastore for table metadata. Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# TODO: Create a SparkSession with Hive support enabled\n",
    "# Hint: Use .enableHiveSupport() and set hive.metastore.uris\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveExercises\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    # TODO: Add enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify connection by listing databases\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Databases and Tables\n",
    "\n",
    "### Exercise 2.1: Create a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a database called 'sales_db'\n",
    "# Hint: Use spark.sql(\"CREATE DATABASE ...\")\n",
    "\n",
    "\n",
    "# Switch to the new database\n",
    "spark.sql(\"USE sales_db\")\n",
    "\n",
    "# Verify\n",
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Create a Managed Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a managed table called 'customers' with the following columns:\n",
    "# - customer_id (INT)\n",
    "# - name (STRING)\n",
    "# - email (STRING)  \n",
    "# - signup_date (DATE)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: Write your CREATE TABLE statement here\n",
    "\"\"\")\n",
    "\n",
    "# Verify table was created\n",
    "spark.sql(\"DESCRIBE customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Create an External Table\n",
    "\n",
    "External tables point to data stored outside Hive's warehouse. The data persists even if the table is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create some sample data in HDFS\n",
    "sample_data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 999.99),\n",
    "    (2, \"Mouse\", \"Electronics\", 29.99),\n",
    "    (3, \"Desk\", \"Furniture\", 299.99),\n",
    "    (4, \"Chair\", \"Furniture\", 199.99),\n",
    "    (5, \"Monitor\", \"Electronics\", 399.99)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sample_data, [\"product_id\", \"name\", \"category\", \"price\"])\n",
    "\n",
    "# Save to HDFS as CSV\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/user/hive/external/products\")\n",
    "\n",
    "print(\"Data written to HDFS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an EXTERNAL table pointing to the CSV data\n",
    "# Hint: Use LOCATION clause and specify ROW FORMAT\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: Create external table 'products' \n",
    "    -- with columns: product_id INT, name STRING, category STRING, price DOUBLE\n",
    "    -- pointing to /user/hive/external/products\n",
    "\"\"\")\n",
    "\n",
    "# Query the external table\n",
    "spark.sql(\"SELECT * FROM products\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Partitioned Tables\n",
    "\n",
    "Partitioning divides data into directories based on column values, improving query performance.\n",
    "\n",
    "### Exercise 3.1: Create a Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a partitioned table for sales data\n",
    "# Partition by: year (INT) and month (INT)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS sales (\n",
    "        sale_id INT,\n",
    "        product_id INT,\n",
    "        quantity INT,\n",
    "        amount DOUBLE,\n",
    "        sale_date DATE\n",
    "    )\n",
    "    -- TODO: Add PARTITIONED BY clause for year and month\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into partitions\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO sales PARTITION (year=2024, month=1)\n",
    "    VALUES (1, 1, 2, 1999.98, '2024-01-15'),\n",
    "           (2, 2, 5, 149.95, '2024-01-20')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO sales PARTITION (year=2024, month=2) \n",
    "    VALUES (3, 3, 1, 299.99, '2024-02-10'),\n",
    "           (4, 4, 3, 599.97, '2024-02-25')\n",
    "\"\"\")\n",
    "\n",
    "# View partitions\n",
    "spark.sql(\"SHOW PARTITIONS sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Query with Partition Pruning\n",
    "\n",
    "When you filter on partition columns, Hive only scans relevant partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Query only January 2024 sales\n",
    "# This should use partition pruning (only read year=2024/month=1 directory)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: Write a SELECT query filtering by year=2024 AND month=1\n",
    "\"\"\").show()\n",
    "\n",
    "# Check the execution plan to see partition pruning\n",
    "spark.sql(\"SELECT * FROM sales WHERE year=2024 AND month=1\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: HiveQL Queries\n",
    "\n",
    "### Exercise 4.1: Aggregation Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate total sales amount per month\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: Write GROUP BY query to sum amount by year, month\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: JOIN Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Join sales with products to show product names with sales\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: Write a JOIN query between sales and products tables\n",
    "    -- Show: product name, category, quantity sold, amount\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Features\n",
    "\n",
    "### Exercise 5.1: Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a window function to calculate running total of sales\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: Calculate cumulative sum of amount ordered by sale_date\n",
    "    -- Hint: Use SUM() OVER (ORDER BY ...)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Create a View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a view that shows monthly sales summary\n",
    "spark.sql(\"\"\"\n",
    "    -- TODO: CREATE VIEW monthly_summary AS ...\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM monthly_summary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up tables (uncomment to run)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS sales\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS products\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS customers\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS sales_db CASCADE\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"Session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

